# ğŸ§¹ ë°ì´í„° ì¡°ì‘ ë° ì „ì²˜ë¦¬ (Data Manipulation & Preprocessing)
**ë°ì´í„° ì¡°ì‘ ë° ì „ì²˜ë¦¬(Data Manipulation & Preprocessing)**  
AI í•™ìŠµ ëª¨ë¸ì— íˆ¬ì…ë˜ê¸° ì „, ì›ì‹œ ë°ì´í„°(Raw Data)ë¥¼ ë¶„ì„í•˜ê³  ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê¹¨ë—í•˜ê³  ìœ ìš©í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •

## ë°ì´í„° ì¡°ì‘ ë° ì „ì²˜ë¦¬ì˜ ê°œë…

| ê°œë… | ì„¤ëª… |
| :--- | :--- |
| **ë°ì´í„° ì¡°ì‘ (Manipulation)** | ë°ì´í„°ë¥¼ ë¶„ì„ ëª©ì ì— ë§ê²Œ ì„ íƒ, ê²°í•©, ë³€í˜•, ì§‘ê³„í•˜ëŠ” ê³¼ì •<br>ì£¼ë¡œ **Pandas** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ DataFrame í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ê°€ê³µ |
| **ë°ì´í„° ì „ì²˜ë¦¬ (Preprocessing)** | ë°ì´í„°ì˜ í’ˆì§ˆì„ ê°œì„ í•˜ê³ , ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì¼ê´€ì„± ì—†ëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì •í•˜ì—¬ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ ìƒíƒœë¡œ ë§Œë“œëŠ” ê³¼ì •<br>ì£¼ë¡œ **ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ ì²˜ë¦¬** ë° **íŠ¹ì„± ìŠ¤ì¼€ì¼ë§**ì„ í¬í•¨ |

---

##  ë°ì´í„° ì „ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ë²•

ì „ì²˜ë¦¬ëŠ” ë°ì´í„°ë¥¼ ê¹¨ë—í•˜ê²Œ ë§Œë“¤ê³ , ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜

### ë°ì´í„° í´ë¦¬ë‹ (Data Cleaning)

| ê¸°ë²• | ì„¤ëª… |
| :--- | :--- |
| **ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Missing Values)** | `NaN`, `None` ë“±ì˜ ë¹„ì–´ìˆëŠ” ê°’ì„ ì œê±°í•˜ê±°ë‚˜ ì ì ˆí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´ |
| **ì´ìƒì¹˜ ì²˜ë¦¬ (Outliers)** | ê°’ì˜ ë²”ìœ„ì—ì„œ ë²—ì–´ë‚œ ì´ìƒì¹˜ë¥¼ ì‚­ì œ ë˜ëŠ” ìˆ˜ì • |
| **ì¤‘ë³µ ë° ì˜¤ë¥˜ ì²˜ë¦¬** | ë™ì¼ ë°ì´í„° ì¤‘ë³µ ì œê±° ë° ë¶€ì ì ˆí•œ ê°’ ìˆ˜ì • |

### ë°ì´í„° ë³€í™˜ ë° ìŠ¤ì¼€ì¼ë§ (Data Transformation & Scaling)

| ê¸°ë²• | ì„¤ëª… |
| :--- | :--- |
| **íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (Scaling)** | ë³€ìˆ˜ ê°’ ë²”ìœ„ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶”ì–´ í•™ìŠµ ì•ˆì •í™” |
| **í‘œì¤€í™” (Standardization)** | í‰ê·  0, í‘œì¤€í¸ì°¨ 1 ë¶„í¬ë¡œ ë³€í™˜ |
| **ì •ê·œí™” (Normalization)** | 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê· ì¼í™” |

### íŠ¹ì„± ì¸ì½”ë”© ë° ê³µí•™ (Encoding & Feature Engineering)

| ê¸°ë²• | ì„¤ëª… |
| :--- | :--- |
| **ë²”ì£¼í˜• ì¸ì½”ë”©** | ë¬¸ìí˜• ë²”ì£¼ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜ |
| **ì›-í•« ì¸ì½”ë”©** | ë²”ì£¼ ìˆ˜ë§Œí¼ ì´ì§„ íŠ¹ì„± ìƒì„± |
| **íŠ¹ì„± ê³µí•™ (Feature Engineering)** | ê¸°ì¡´ íŠ¹ì„±ì„ ì¡°í•©Â·ë³€í˜•í•´ ì˜ë¯¸ ìˆëŠ” íŠ¹ì„± ìƒì„± |

---

## ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì—­í• 

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì£¼ìš” ì—­í•  |
| :--- | :--- |
| **Pandas** | ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°, ê°€ê³µ, ê²°ì¸¡ì¹˜ ì²˜ë¦¬ |
| **NumPy** | ìˆ˜ì¹˜ ê³„ì‚° ë° ë°°ì—´ ì—°ì‚° |
| **Scikit-learn** | ë°ì´í„° ì „ì²˜ë¦¬ ë° ì „í†µ ML ëª¨ë¸ êµ¬ì„± |
| **TensorFlow / Keras** | ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬í˜„, í•™ìŠµ, í‰ê°€ |

---

# ë”¥ëŸ¬ë‹ í•™ìŠµ ê³¼ì •

## 1. ë°ì´í„° ì¤€ë¹„ ë° í™•ì¸

- ë°ì´í„°ì…‹ ë¡œë“œ ë° êµ¬ì¡° í™•ì¸ (ì˜ˆ: MNIST ì´ë¯¸ì§€ í¬ê¸° ë° ë¼ë²¨ í™•ì¸)
- ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”ë¡œ ë‚´ìš© ì´í•´
- ì •ê·œí™”, ì±„ë„ ì°¨ì› ì¶”ê°€, ì›-í•« ì¸ì½”ë”© ì²˜ë¦¬

```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
print("Train data shape:", X_train.shape, y_train.shape)
plt.imshow(X_train, cmap='gray')
plt.title(f"Label: {y_train}")
plt.show()

X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0
X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]

y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
```

## 2. CNN ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ

- ì»¨ë³¼ë£¨ì…˜, í’€ë§, ì™„ì „ ì—°ê²°ì¸µìœ¼ë¡œ êµ¬ì„±ëœ CNN ëª¨ë¸ êµ¬í˜„
- ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì • í›„ í•™ìŠµ

```
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)
```

## 3. AlexNet êµ¬ì¡° ë³€í˜• ëª¨ë¸ ì ìš© ë° í•™ìŠµ

- AlexNetì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œë¥¼ ì¶•ì†Œí•´ ë³€í˜• ì ìš©
- ë“œë¡­ì•„ì›ƒ í¬í•¨í•˜ì—¬ ê³¼ì í•© ë°©ì§€ ì‹œë„

```
model = models.Sequential([
    layers.Conv2D(64, 3, strides=1, padding='same', activation='relu', input_shape=(28,28,1)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(192, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(384, 3, padding='same', activation='relu'),
    layers.Conv2D(256, 3, padding='same', activation='relu'),
    layers.Conv2D(256, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(4096, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(4096, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)
```

## 4. í‰ê°€ ë° ì‹œê°í™”

- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€
- í•™ìŠµ ì •í™•ë„ ë° ê²€ì¦ ì •í™•ë„ ì¶”ì´ ê·¸ë˜í”„ ì¶œë ¥

```
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```
